{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604dcc78",
   "metadata": {},
   "source": [
    "참고자료 https://thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b62280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c514870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroup = pd.read_csv('C:/Users/user/Desktop/bilm-tf-master/20news_dataset_clear/20newsgroup_preprocessed.csv', sep=';', usecols=['target', 'text_cleaned'])\n",
    "df_newsgroup.rename(columns={'text_cleaned' : 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c55261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df_newsgroup['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de877ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroup['target'] = le.transform(df_newsgroup['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fca4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_newsgroup['text'].astype(str)\n",
    "# y = tf.keras.utils.to_categorical(df_newsgroup['target'], num_classes=df_newsgroup['target'].nunique())\n",
    "y=list(df_newsgroup['target'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_newsgroup['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f18ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=list(X_train)\n",
    "x_test=list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc607c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = '[^a-z.0-9 ]'\n",
    "clr_x_trian=[]\n",
    "clr_x_test=[]\n",
    "for sen in x_train:\n",
    "    clr_x_trian.append(re.sub(pattern, '', sen))\n",
    "for sen in x_test:\n",
    "    clr_x_test.append(re.sub(pattern, '', sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f54eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=clr_x_trian\n",
    "x_test=clr_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad78d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_txt_filter=x_train+x_test\n",
    "y=y_train+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3174ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'text': to_txt_filter\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': x_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a8073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "train_df = Dataset.from_pandas(train_df)\n",
    "test_df = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97abcff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen various references triple des recently could anyone tell context would guess means encrypting block times different key time id like sure replies email preferred news unreliable thanks richard pgp public key available request pgp public key available request disclaimer please note personal view construed official comment jet project\n",
      "==================================================\n",
      "j r laferriere wrote wondering law officers read several questions would like ask pertaining motorcycles cops please dont say get vehicle code go local station obvious things like questions would found places answered face face real live flesh cop brother friend cousin whos father cop etc dont bother writing thanks um would mind telling us statemunicipality live different laws different places know tom coradeschi usenet like herd performing elephants diarrhea massive difficult redirect aweinspiring entertaining source mind boggling amounts excrement least expect gene spafford\n",
      "==================================================\n",
      "sandy santra writes mike mattone wrote ive computer months thats long time reasonable life cycle lcd display think months nothing wrong quite reasonable bought compaq toshiba might reasonably expected machine last longer something went wrong thats moot point perhaps maybe ive epson portable backlit lcd since still used daily screen fine problem ever arm screen sorta lose bend fairly harshly screen goes wiggle round bit perfect whats um months justin sullivan system administrator dialix services sydney modem ph perth\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for t in train_df['text'][:3]:\n",
    "  print(t)\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7356765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have your custom dataset\n",
    "# dataset = LineByLineTextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=\"path/to/data.txt\",\n",
    "#     block_size=64,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93d9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if you have huge custom dataset separated into files\n",
    "# load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23de2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer\n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\", encoding='utf-8') as f:\n",
    "    for t in dataset[\"text\"]:\n",
    "      print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(train_df, \"train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(test_df, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "520a1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "524816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db7394b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained-bert\"\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c91a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretrained-bert\\\\vocab.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "tokenizer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eed7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef464f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file pretrained-bert\\tokenizer.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b02121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "# tokenizing the train dataset\n",
    "train_dataset = train_df.map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = test_df.map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0da91ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 18828\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "404e926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dd4930c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18828, 3766)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13c14775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67800e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e61e8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 100 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a32cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d753f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18828\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 2350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2350' max='2350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2350/2350 1:23:25, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.508400</td>\n",
       "      <td>8.069098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.015400</td>\n",
       "      <td>7.848805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3766\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert\\checkpoint-1000\n",
      "Configuration saved in pretrained-bert\\checkpoint-1000\\config.json\n",
      "Model weights saved in pretrained-bert\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3766\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert\\checkpoint-2000\n",
      "Configuration saved in pretrained-bert\\checkpoint-2000\\config.json\n",
      "Model weights saved in pretrained-bert\\checkpoint-2000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2350, training_loss=8.21013713430851, metrics={'train_runtime': 5008.813, 'train_samples_per_second': 37.59, 'train_steps_per_second': 0.469, 'total_flos': 4.95488324192256e+16, 'train_loss': 8.21013713430851, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80a3c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pretrained-bert\\checkpoint-2000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pretrained-bert\\checkpoint-2000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at pretrained-bert\\checkpoint-2000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file pretrained-bert\\tokenizer.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when you load from pretrained\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-2000\"))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "# or simply use pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c73f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.016480499878525734, 'token': 43, 'token_str': '##s', 'sequence': 'it is known thats is the capital of germany'}\n",
      "{'score': 0.006546618416905403, 'token': 261, 'token_str': 'new', 'sequence': 'it is known that new is the capital of germany'}\n",
      "{'score': 0.006186113692820072, 'token': 744, 'token_str': 'university', 'sequence': 'it is known that university is the capital of germany'}\n",
      "{'score': 0.00437399186193943, 'token': 39, 'token_str': '##n', 'sequence': 'it is known thatn is the capital of germany'}\n",
      "{'score': 0.004100380465388298, 'token': 186, 'token_str': 'one', 'sequence': 'it is known that one is the capital of germany'}\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "example = \"It is known that [MASK] is the capital of Germany\"\n",
    "for prediction in fill_mask(example):\n",
    "  print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0acaa0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today s most trending hashtags ons is donald trump, confidence: 0.007189965341240168\n",
      "today s most trending hashtags onn is donald trump, confidence: 0.003860343713313341\n",
      "today s most trending hashtags on new is donald trump, confidence: 0.0031429044902324677\n",
      "today s most trending hashtags one is donald trump, confidence: 0.0030795179773122072\n",
      "today s most trending hashtags ona is donald trump, confidence: 0.0023588649928569794\n",
      "==================================================\n",
      "thes was cloudy yesterday but today it s rainy, confidence: 0.005863609258085489\n",
      "then was cloudy yesterday but today it s rainy, confidence: 0.00403449684381485\n",
      "the new was cloudy yesterday but today it s rainy, confidence: 0.00355438026599586\n",
      "thea was cloudy yesterday but today it s rainy, confidence: 0.002700341632589698\n",
      "thee was cloudy yesterday but today it s rainy, confidence: 0.0024629526305943727\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "  for prediction in fill_mask(example):\n",
    "    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dab96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  3 14:18:08 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.92                 Driver Version: 545.92       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 39%   55C    P8              30W / 350W |  24150MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2516    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      3036    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A      8860    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9728    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10108    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     12748    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13792      C   ...user\\anaconda3\\envs\\bert\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     15456    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16972    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18528    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     19192    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     20064    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     32928    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     33328    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe    N/A      |\n",
      "|    0   N/A  N/A     38048    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ca5ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pretrained-bert\\checkpoint-2000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pretrained-bert\\checkpoint-2000\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at pretrained-bert\\checkpoint-2000 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pretrained-bert\\checkpoint-2000 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(model_path, \"checkpoint-2000\"), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "169a1495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file pretrained-bert\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47ea7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers, _ = model(tokens_tensor, segments_tensors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7fa5eea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "text = to_txt_filter[5]\n",
    "\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(marked_text) #서브토큰화\n",
    "\n",
    "indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Python list를 PyTorch tensor로 변환하기 \n",
    "tokens_tensor = torch.tensor([indexed_text])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_model = model(tokens_tensor, segments_tensors )\n",
    "    hidden_layers = bert_model[1][1:]     #13개의 값 중 첫번째 요소는 최초 임베딩 그러므로 뒤의 12개 은닉 상태만 얻자\n",
    "    \n",
    "token_embeddings = torch.stack(hidden_layers, dim=0) #12개 레이어 쌓기 # torch.Size([12, 1, 22, 768])\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1) #배치 차원 제거 # torch.Size([12, 22, 768])\n",
    "token_embeddings = token_embeddings.permute(1,0,2) #위치 변환 # torch.Size([22, 12, 768])\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    token_vecs_sum.append(np.array(sum_vec))\n",
    "    \n",
    "\n",
    "token_vecs_sum=np.array(token_vecs_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffb596e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_continuous_integers(lst):                                      #연속된 정수리스트를 split 해주는 함수\n",
    "    for k, g in groupby(enumerate(lst), lambda i_x:i_x[0]-i_x[1]):\n",
    "        yield list(map(itemgetter(1), g))\n",
    "        \n",
    "def add_previous_number(lst):                                            #최초 서브토큰 인덱스 추가\n",
    "    return [[sub_lst[0] - 1] + sub_lst for sub_lst in lst]\n",
    "\n",
    "def bert_word_embedding(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text) #서브토큰화\n",
    "    indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    # Python list를 PyTorch tensor로 변환하기 \n",
    "    tokens_tensor = torch.tensor([indexed_text])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        bert_model = model(tokens_tensor, segments_tensors )\n",
    "        hidden_layers = bert_model[1][1:]     #13개의 값 중 첫번째 요소는 최초 임베딩 그러므로 뒤의 12개 은닉 상태만 얻자\n",
    "    token_embeddings = torch.stack(hidden_layers, dim=0) #12개 레이어 쌓기 # torch.Size([12, 1, 22, 768])\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1) #배치 차원 제거 # torch.Size([12, 22, 768])\n",
    "    token_embeddings = token_embeddings.permute(1,0,2) #위치 변환 # torch.Size([22, 12, 768])\n",
    "    \n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:                #인코더의 마지막 4개의 은닉 상태를 합쳐 최종 벡터(4개 합친 것이 성능이 가장 좋음)\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(np.array(sum_vec))\n",
    "    \n",
    "    token_vecs_sum=np.array(token_vecs_sum)\n",
    "    #서브토큰을 결합해 단어에 대한 임베딩 벡터를 얻자\n",
    "    subword_indices = [i for i, token in enumerate(tokenized_text) if '##' in token]\n",
    "    index_list = add_previous_number(list(split_continuous_integers(subword_indices))) #index_list는 서브토큰에 해당하는 인덱스를 한 리스트에 묶어줌\n",
    "    \n",
    "    new_token_vecs_sum = []\n",
    "    last_index = 0\n",
    "\n",
    "    for subword_inx_list in index_list:\n",
    "        # 이전 인덱스와 현재 인덱스 그룹 사이의 벡터를 추가합니다.\n",
    "        new_token_vecs_sum.extend(token_vecs_sum[last_index:subword_inx_list[0]])\n",
    "\n",
    "        # 현재 인덱스 그룹에 해당하는 벡터의 평균을 계산하고 추가합니다.\n",
    "        avg_vecs = np.mean(token_vecs_sum[subword_inx_list], axis=0)\n",
    "        new_token_vecs_sum.append(avg_vecs)\n",
    "\n",
    "        # 마지막 인덱스를 업데이트합니다.\n",
    "        last_index = subword_inx_list[-1] + 1\n",
    "\n",
    "    # 마지막 인덱스 이후의 벡터를 추가합니다.\n",
    "    new_token_vecs_sum.extend(token_vecs_sum[last_index:])\n",
    "\n",
    "    # 결과를 numpy array로 변환합니다.\n",
    "    new_token_vecs_sum = np.array(new_token_vecs_sum[1:-1])\n",
    "    \n",
    "    return new_token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7be0962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 768)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs_sum=bert_word_embedding(text)\n",
    "token_vecs_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a6a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef2062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cb14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de1551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19aafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31872be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b34d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67821f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92ea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d05072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7221cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e361c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b03456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429cccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731988b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16da76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b04268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e74d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e0193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8983e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
