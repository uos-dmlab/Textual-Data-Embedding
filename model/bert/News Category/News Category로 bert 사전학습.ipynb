{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd1608c",
   "metadata": {},
   "source": [
    "참고자료 https://thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b62280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753bd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('C:/Users/user/Desktop/bilm-tf-master/textdataset/News_Category_Dataset_v3.json', 'r') as f:\n",
    "    for line in f:\n",
    "        content = json.loads(line)\n",
    "        data.append(content)\n",
    "df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a58a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189815, 6)\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['short_description'].apply(lambda x: len(x)==0)]\n",
    "print(df.shape)\n",
    "\n",
    "#중복되는 카테고리 통합\n",
    "df[\"category\"] = df.category.replace(\n",
    "    {\n",
    "        \"THE WORLDPOST\": \"WORLD NEWS\",\n",
    "        \"WORLDPOST\": \"WORLD NEWS\",\n",
    "        \"ARTS\": \"ARTS & CULTURE\",\n",
    "        \"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
    "        \"HEALTHY LIVING\": \"WELLNESS\",\n",
    "        \"QUEER VOICES\": \"VOICES\",\n",
    "        \"LATINO VOICES\": \"VOICES\",\n",
    "        \"BLACK VOICES\": \"VOICES\",\n",
    "        \"STYLE\": \"STYLE & BEATUY\",\n",
    "        \"GREEN\": \"ENVIRONMENT\",\n",
    "        \"TASTE\": \"FOOD & DRINK\",\n",
    "        \"MONEY\": \"BUSINESS\",\n",
    "        \"PARENTING\": \"PARENTS\"\n",
    "    }\n",
    ")\n",
    "\n",
    "df = df[['headline', 'short_description','category']]\n",
    "df['input_data']= df.apply(lambda x: str(x['headline']) + str(x['short_description']), axis=1)\n",
    "\n",
    "def clean_text(text):\n",
    "    text=str(text).lower() #Converts text to lowercase\n",
    "    text=re.sub('\\d+', '', text) #removes numbers\n",
    "    text=re.sub('\\[.*?\\]', '', text) #removes HTML tags\n",
    "    text=re.sub('https?://\\S+|www\\.\\S+', '', text) #removes url\n",
    "    text=re.sub(r\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", \"\", text) #removes emojis\n",
    "    text=re.sub('[%s]' % re.escape(string.punctuation),'',text) #removes punctuations\n",
    "    #text = re.sub('\\n', '', text)\n",
    "    #text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df['clean_text']=df['input_data'].apply(clean_text)\n",
    "\n",
    "class0=list(df[df['category'] == 'POLITICS'].sample(1000, random_state=1)['clean_text'])\n",
    "class1=list(df[df['category'] == 'WELLNESS'].sample(1000, random_state=1)['clean_text'])\n",
    "class2=list(df[df['category'] == 'ENTERTAINMENT'].sample(1000, random_state=1)['clean_text'])\n",
    "class3=list(df[df['category'] == 'PARENTS'].sample(1000, random_state=1)['clean_text'])\n",
    "class4=list(df[df['category'] == 'VOICES'].sample(1000, random_state=1)['clean_text'])\n",
    "class5=list(df[df['category'] == 'STYLE & BEAUTY'].sample(1000, random_state=1)['clean_text'])\n",
    "class6=list(df[df['category'] == 'TRAVEL'].sample(1000, random_state=1)['clean_text'])\n",
    "class7=list(df[df['category'] == 'FOOD & DRINK'].sample(1000, random_state=1)['clean_text'])\n",
    "class8=list(df[df['category'] == 'WORLD NEWS'].sample(1000, random_state=1)['clean_text'])\n",
    "class9=list(df[df['category'] == 'BUSINESS'].sample(1000, random_state=1)['clean_text'])\n",
    "\n",
    "X_list=class0+class1+class2+class3+class4+class5+class6+class7+class8+class9\n",
    "\n",
    "y=[]\n",
    "for i in range(10):\n",
    "    y+=[i]*1000\n",
    "\n",
    "clr_x_data=[]\n",
    "pattern = '[^a-z ]'\n",
    "for sen in X_list:\n",
    "    clr_x_data.append(re.sub(pattern, ' ', sen))\n",
    "\n",
    "X_list=[]\n",
    "for sen in clr_x_data:\n",
    "    X_list.append(' '.join(sen.split()))\n",
    "\n",
    "train_idx=[]\n",
    "for i in range(10):\n",
    "    train_idx+=[j+i*1000 for j in range(800)]\n",
    "\n",
    "test_idx=[]\n",
    "for i in range(10):\n",
    "    test_idx+=[j+800+i*1000 for j in range(200)]\n",
    "\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "for i in train_idx:\n",
    "    x_train.append(X_list[i])\n",
    "    y_train.append(y[i])\n",
    "\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for i in test_idx:\n",
    "    x_test.append(X_list[i])\n",
    "    y_test.append(y[i])\n",
    "\n",
    "to_txt=x_train+x_test\n",
    "y=list(y_train)+list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd80cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "\n",
    "encoder.fit(y)\n",
    "\n",
    "label=encoder.transform(y)\n",
    "\n",
    "y_train=label[:8000]\n",
    "y_test=label[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad78d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_txt_filter=x_train+x_test\n",
    "y=list(y_train)+list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3174ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'text': to_txt_filter\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': x_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a8073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "train_df = Dataset.from_pandas(train_df)\n",
    "test_df = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97abcff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gop congressman trump should apologize to obama over wiretapping claimsthe comments from oklahomas tom cole are the strongest republican admonition of trumps baseless allegations to date\n",
      "==================================================\n",
      "the gop tax plan is not as simple as they saya business tax cut could open a big loophole for wealthy filers to use\n",
      "==================================================\n",
      "obama says us should be proud of economic progressa new report released on friday showed an eightyearlow in unemployment\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for t in train_df['text'][:3]:\n",
    "  print(t)\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7356765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have your custom dataset\n",
    "# dataset = LineByLineTextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=\"path/to/data.txt\",\n",
    "#     block_size=64,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93d9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if you have huge custom dataset separated into files\n",
    "# load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23de2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer\n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\", encoding='utf-8') as f:\n",
    "    for t in dataset[\"text\"]:\n",
    "      print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(train_df, \"train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(test_df, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "520a1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7394b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained-bert-768\"\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6c91a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretrained-bert-768\\\\vocab.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "tokenizer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eed7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef464f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file pretrained-bert-768\\tokenizer.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert-768\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert-768\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert-768\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained-bert-768\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert-768\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b02121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "# tokenizing the train dataset\n",
    "train_dataset = train_df.map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = test_df.map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0da91ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "404e926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd4930c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13c14775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, hidden_size = 768, max_position_embeddings=max_length) #은닉 상태의 크기 조절\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67800e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e61e8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1250\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=200,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=1250,             # evaluate, log and save model checkpoints every 100 step\n",
    "    save_steps=1250,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a32cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d753f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\user\\anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 14:19:41, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>7.515300</td>\n",
       "      <td>7.106977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.977100</td>\n",
       "      <td>6.777702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>6.672600</td>\n",
       "      <td>6.390101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.267800</td>\n",
       "      <td>5.864687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>5.752000</td>\n",
       "      <td>5.303067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.304300</td>\n",
       "      <td>4.906940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>4.910500</td>\n",
       "      <td>4.446493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.578200</td>\n",
       "      <td>4.093762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>4.269400</td>\n",
       "      <td>3.843916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.985800</td>\n",
       "      <td>3.494203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>3.744500</td>\n",
       "      <td>3.225507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.518300</td>\n",
       "      <td>2.968833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>3.310400</td>\n",
       "      <td>2.769131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.139600</td>\n",
       "      <td>2.595969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>2.990100</td>\n",
       "      <td>2.461854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.866800</td>\n",
       "      <td>2.319963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>2.760200</td>\n",
       "      <td>2.233273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.665600</td>\n",
       "      <td>2.141250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>2.610600</td>\n",
       "      <td>2.107873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.573100</td>\n",
       "      <td>2.100831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-1250\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-1250\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-1250\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-2500\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-2500\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-2500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-3750\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-3750\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-3750\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-5000\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-5000\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-6250\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-6250\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-6250\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-7500\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-7500\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-7500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-8750\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-8750\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-8750\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-10000\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-10000\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-10000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-11250\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-11250\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-11250\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-12500\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-12500\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-12500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-13750\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-13750\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-13750\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-15000\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-15000\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-15000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-16250\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-16250\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-16250\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-17500\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-17500\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-17500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-18750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in pretrained-bert-768\\checkpoint-18750\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-18750\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-20000\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-20000\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-20000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-21250\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-21250\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-21250\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-22500\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-22500\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-22500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-23750\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-23750\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-23750\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to pretrained-bert-768\\checkpoint-25000\n",
      "Configuration saved in pretrained-bert-768\\checkpoint-25000\\config.json\n",
      "Model weights saved in pretrained-bert-768\\checkpoint-25000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25000, training_loss=4.320608798828125, metrics={'train_runtime': 51585.2746, 'train_samples_per_second': 38.771, 'train_steps_per_second': 0.485, 'total_flos': 5.264096256e+17, 'train_loss': 4.320608798828125, 'epoch': 200.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train() #resume_from_checkpoint=\"path_to_your_checkpoint\"으로 체크포인트부터 다시 학습가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c80a3c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pretrained-bert-768\\checkpoint-25000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pretrained-bert-768\\checkpoint-25000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at pretrained-bert-768\\checkpoint-25000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Didn't find file pretrained-bert-768\\tokenizer.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert-768\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert-768\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert-768\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained-bert-768\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert-768\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when you load from pretrained\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-25000\"))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "# or simply use pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c73f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6728640794754028, 'token': 1870, 'token_str': 'germany', 'sequence': 'it is known that germany is the capital of germany'}\n",
      "{'score': 0.03188249468803406, 'token': 176, 'token_str': 'it', 'sequence': 'it is known that it is the capital of germany'}\n",
      "{'score': 0.014234239235520363, 'token': 607, 'token_str': 'europe', 'sequence': 'it is known that europe is the capital of germany'}\n",
      "{'score': 0.012257604859769344, 'token': 1948, 'token_str': 'german', 'sequence': 'it is known that german is the capital of germany'}\n",
      "{'score': 0.006740014534443617, 'token': 1912, 'token_str': 'britain', 'sequence': 'it is known that britain is the capital of germany'}\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "example = \"It is known that [MASK] is the capital of Germany\"\n",
    "for prediction in fill_mask(example):\n",
    "  print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0acaa0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today s most trending hashtags on australia is donald trump, confidence: 0.028284719213843346\n",
      "today s most trending hashtags on india is donald trump, confidence: 0.02640661969780922\n",
      "today s most trending hashtags on news is donald trump, confidence: 0.021525299176573753\n",
      "today s most trending hashtags on focus is donald trump, confidence: 0.017980456352233887\n",
      "today s most trending hashtags on the is donald trump, confidence: 0.010508195497095585\n",
      "==================================================\n",
      "the season was cloudy yesterday but today it s rainy, confidence: 0.052380044013261795\n",
      "the ball was cloudy yesterday but today it s rainy, confidence: 0.010524711571633816\n",
      "the game was cloudy yesterday but today it s rainy, confidence: 0.010513330809772015\n",
      "the bowl was cloudy yesterday but today it s rainy, confidence: 0.009463183581829071\n",
      "the loss was cloudy yesterday but today it s rainy, confidence: 0.00937189906835556\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "  for prediction in fill_mask(example):\n",
    "    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 10:54:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.92                 Driver Version: 545.92       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 35%   26C    P8              27W / 350W |  23512MiB / 24576MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       824    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      5196    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      6068      C   C:\\Users\\user\\anaconda3\\python.exe        N/A      |\n",
      "|    0   N/A  N/A      7436    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8236      C   ...user\\anaconda3\\envs\\bert\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      9340    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     13336    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13660    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15784    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     17020    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18524    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20828    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     23088    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     24300    C+G   ...t Office\\root\\Office16\\POWERPNT.EXE    N/A      |\n",
      "|    0   N/A  N/A     24916    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     25496    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     26688    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ca5ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pretrained-bert-768\\checkpoint-25000\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pretrained-bert-768\\checkpoint-25000\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at pretrained-bert-768\\checkpoint-25000 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pretrained-bert-768\\checkpoint-25000 and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(model_path, \"checkpoint-25000\"), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "169a1495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file pretrained-bert-768\\added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained-bert-768\\tokenizer_config.json. We won't load it.\n",
      "loading file pretrained-bert-768\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained-bert-768\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert-768\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fa5eea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "text = to_txt_filter[5]\n",
    "\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(marked_text) #서브토큰화\n",
    "\n",
    "indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Python list를 PyTorch tensor로 변환하기 \n",
    "tokens_tensor = torch.tensor([indexed_text])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_model = model(tokens_tensor, segments_tensors )\n",
    "    hidden_layers = bert_model[1][1:]     #13개의 값 중 첫번째 요소는 최초 임베딩 그러므로 뒤의 12개 은닉 상태만 얻자\n",
    "    \n",
    "token_embeddings = torch.stack(hidden_layers, dim=0) #12개 레이어 쌓기 # torch.Size([12, 1, 22, 768])\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1) #배치 차원 제거 # torch.Size([12, 22, 768])\n",
    "token_embeddings = token_embeddings.permute(1,0,2) #위치 변환 # torch.Size([22, 12, 768])\n",
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    token_vecs_sum.append(np.array(sum_vec))\n",
    "    \n",
    "\n",
    "token_vecs_sum=np.array(token_vecs_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffb596e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_continuous_integers(lst):                                      #연속된 정수리스트를 split 해주는 함수\n",
    "    for k, g in groupby(enumerate(lst), lambda i_x:i_x[0]-i_x[1]):\n",
    "        yield list(map(itemgetter(1), g))\n",
    "        \n",
    "def add_previous_number(lst):                                            #최초 서브토큰 인덱스 추가\n",
    "    return [[sub_lst[0] - 1] + sub_lst for sub_lst in lst]\n",
    "\n",
    "def bert_word_embedding(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text) #서브토큰화\n",
    "    indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    # Python list를 PyTorch tensor로 변환하기 \n",
    "    tokens_tensor = torch.tensor([indexed_text])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        bert_model = model(tokens_tensor, segments_tensors )\n",
    "        hidden_layers = bert_model[1][1:]     #13개의 값 중 첫번째 요소는 최초 임베딩 그러므로 뒤의 12개 은닉 상태만 얻자\n",
    "    token_embeddings = torch.stack(hidden_layers, dim=0) #12개 레이어 쌓기 # torch.Size([12, 1, 22, 768])\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1) #배치 차원 제거 # torch.Size([12, 22, 768])\n",
    "    token_embeddings = token_embeddings.permute(1,0,2) #위치 변환 # torch.Size([22, 12, 768])\n",
    "    \n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:                #인코더의 마지막 4개의 은닉 상태를 합쳐 최종 벡터(4개 합친 것이 성능이 가장 좋음)\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(np.array(sum_vec))\n",
    "    \n",
    "    token_vecs_sum=np.array(token_vecs_sum)\n",
    "    #서브토큰을 결합해 단어에 대한 임베딩 벡터를 얻자\n",
    "    subword_indices = [i for i, token in enumerate(tokenized_text) if '##' in token]\n",
    "    index_list = add_previous_number(list(split_continuous_integers(subword_indices))) #index_list는 서브토큰에 해당하는 인덱스를 한 리스트에 묶어줌\n",
    "    \n",
    "    new_token_vecs_sum = []\n",
    "    last_index = 0\n",
    "\n",
    "    for subword_inx_list in index_list:\n",
    "        # 이전 인덱스와 현재 인덱스 그룹 사이의 벡터를 추가합니다.\n",
    "        new_token_vecs_sum.extend(token_vecs_sum[last_index:subword_inx_list[0]])\n",
    "\n",
    "        # 현재 인덱스 그룹에 해당하는 벡터의 평균을 계산하고 추가합니다.\n",
    "        avg_vecs = np.mean(token_vecs_sum[subword_inx_list], axis=0)\n",
    "        new_token_vecs_sum.append(avg_vecs)\n",
    "\n",
    "        # 마지막 인덱스를 업데이트합니다.\n",
    "        last_index = subword_inx_list[-1] + 1\n",
    "\n",
    "    # 마지막 인덱스 이후의 벡터를 추가합니다.\n",
    "    new_token_vecs_sum.extend(token_vecs_sum[last_index:])\n",
    "\n",
    "    # 결과를 numpy array로 변환합니다.\n",
    "    new_token_vecs_sum = np.array(new_token_vecs_sum[1:-1])\n",
    "    \n",
    "    return new_token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7be0962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs_sum=bert_word_embedding(text)\n",
    "token_vecs_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a6a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef2062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cb14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de1551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19aafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31872be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b34d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67821f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92ea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d05072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7221cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e361c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b03456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429cccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731988b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16da76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b04268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e74d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e0193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8983e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
