{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184f59ff",
   "metadata": {},
   "source": [
    "# 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849c1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import json\n",
    "import keras\n",
    "import wordninja\n",
    "from data_preprocessing import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7749ee1",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ec05e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>edge</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>champion product approv stock split champion p...</td>\n",
       "      <td>champion product approv stock split champion p...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comput termin system cpml complet sale comput ...</td>\n",
       "      <td>comput termin system cpml complet sale comput ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n",
       "      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n",
       "      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brown forman inc bfd qtr net shr dlr ct net ml...</td>\n",
       "      <td>brown forman inc bfd qtr net shr dlr ct net ml...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  champion product approv stock split champion p...   \n",
       "1  comput termin system cpml complet sale comput ...   \n",
       "2  cobanco inc cbco year net shr ct dlr net asset...   \n",
       "3  intern inc qtr jan oper shr loss two ct profit...   \n",
       "4  brown forman inc bfd qtr net shr dlr ct net ml...   \n",
       "\n",
       "                                                edge intent  \n",
       "0  champion product approv stock split champion p...   earn  \n",
       "1  comput termin system cpml complet sale comput ...    acq  \n",
       "2  cobanco inc cbco year net shr ct dlr net asset...   earn  \n",
       "3  intern inc qtr jan oper shr loss two ct profit...   earn  \n",
       "4  brown forman inc bfd qtr net shr dlr ct net ml...   earn  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('C:/Users/User/Desktop/r8/r8_dataset/r8-train-stemmed.csv')\n",
    "valid_df = pd.read_csv('C:/Users/User/Desktop/r8/r8_dataset/r8-dev-stemmed.csv')\n",
    "test_df = pd.read_csv('C:/Users/User/Desktop/r8/r8_dataset/r8-test-stemmed.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92036253-0514-464e-b0c8-90385df7a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df와 valid_df 결합\n",
    "train_val_df = pd.concat([train_df, valid_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9defe64",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dae748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['text']=train_val_df['text'].apply(clean_text)\n",
    "test_df['text']=test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbe03fe-e525-41bb-8365-a83cb4735fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=list(train_val_df['text'])\n",
    "y_train=list(train_val_df['intent'])\n",
    "x_test=list(test_df['text'])\n",
    "y_test=list(test_df['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d07133-8799-4a0c-aa43-51d184676fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_train+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae7bd9d-197d-4d26-a3bb-a42c1983e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(y)\n",
    "label=encoder.transform(y)\n",
    "\n",
    "y_train=list(label[:5484])\n",
    "y_test=list(label[5484:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4430e888-1793-4c03-b915-e33283b693b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_txt=x_train+x_test\n",
    "\n",
    "#사전학습된 버트는 최대 임베딩 토큰 수(512)가 정해져 있으므로 넉넉하게 최대길이를 300으로 제한\n",
    "to_txt=limit_words(to_txt,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1a6e4",
   "metadata": {},
   "source": [
    "# tensor space model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7ec27d-94e2-410d-913d-0d08206cd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 불러오기\n",
    "with open('C:/Users/user/Desktop/english.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c0c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\class\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'daren', 'hadn', 'herse', 'himse', 'itse', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#문서 분류에 도움되는 10,000개 단어 선별\n",
    "vect = CountVectorizer(stop_words=stopwords)\n",
    "X_dtm = vect.fit_transform(to_txt)\n",
    "X_dtm = X_dtm.toarray()\n",
    "X_new = SelectKBest(chi2, k=10000).fit(X_dtm, y)\n",
    "TorF = X_new.get_support()\n",
    "TorF\n",
    "word_view=np.array(vect.get_feature_names_out())\n",
    "feature_lst10000=word_view[TorF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4efe4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data에서 가장 많은 feature를 가진 문서의 경우 feature 169  개를 가짐\n",
      "Train_data에서 가장 적은 feature를 가진 문서의 경우 feature 2  개를 가짐\n",
      "Test_data에서 가장 많은 feature를 가진 문서의 경우 feature 168  개를 가짐\n",
      "Test_data에서 가장 적은 feature를 가진 문서의 경우 feature 3  개를 가짐\n"
     ]
    }
   ],
   "source": [
    "x_train=to_txt[:5484]\n",
    "x_test=to_txt[5484:]\n",
    "\n",
    "doc_lst=[]\n",
    "for sen in x_train:\n",
    "    doc_lst.append(sen.split())\n",
    "\n",
    "test_lst=[]\n",
    "for sen in x_test:\n",
    "    test_lst.append(sen.split())\n",
    "    \n",
    "#Train_data에서 문서가 갖고 있는 선별한 feauture의 수 확인\n",
    "count_lst=[]\n",
    "for i in range(5484):\n",
    "    total_feature_cnt=0\n",
    "    for j in range(10000):\n",
    "        if feature_lst10000[j] in doc_lst[i]:\n",
    "            total_feature_cnt+=1\n",
    "    count_lst.append(total_feature_cnt)\n",
    "    \n",
    "print('Train_data에서 가장 많은 feature를 가진 문서의 경우 feature',max(count_lst),' 개를 가짐')\n",
    "print('Train_data에서 가장 적은 feature를 가진 문서의 경우 feature',min(count_lst),' 개를 가짐')\n",
    "\n",
    "train_max_feature=max(count_lst)\n",
    "\n",
    "#Test_data에서 문서가 갖고 있는 선별한 feauture의 수 확인\n",
    "count_lst=[]\n",
    "for i in range(2189):\n",
    "    \n",
    "    total_feature_cnt=0\n",
    "    for j in range(10000):\n",
    "        if feature_lst10000[j] in test_lst[i]:\n",
    "            total_feature_cnt+=1\n",
    "    count_lst.append(total_feature_cnt)\n",
    "    \n",
    "print('Test_data에서 가장 많은 feature를 가진 문서의 경우 feature',max(count_lst),' 개를 가짐')\n",
    "print('Test_data에서 가장 적은 feature를 가진 문서의 경우 feature',min(count_lst),' 개를 가짐')\n",
    "\n",
    "test_max_feature=max(count_lst)\n",
    "\n",
    "max_feature=max(train_max_feature,test_max_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7fd4e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead9a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Channel TextCuboid 생성\n",
    "textcuboid=[]\n",
    "for i in range(5484):\n",
    "    frame1=np.zeros((max_feature,768)) #(max_feature, 768)\n",
    "    idx_cnt=0\n",
    "    for j in range(10000):\n",
    "        if feature_lst10000[j] in doc_lst[i]:\n",
    "            #문서에서 선별한 단어(feature)의 위치를 찾아 임베딩 벡터 추출\n",
    "            frame1[idx_cnt]=np.load('./bert_embedding/train(bert)/doc%d.npy'%i)[doc_lst[i].index(feature_lst10000[j])]\n",
    "            idx_cnt+=1\n",
    "    textcuboid.append(frame1)\n",
    "textcuboid=np.array(textcuboid)\n",
    "np.save('./1-Channel textcuboid_r8(bert).npy',textcuboid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e75b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Channel TextCuboid 생성\n",
    "textcuboid_test=[]\n",
    "for i in range(2189):\n",
    "    frame1=np.zeros((max_feature,768)) #(max_feature, 768)\n",
    "    idx_cnt=0\n",
    "    for j in range(10000):\n",
    "        if feature_lst10000[j] in test_lst[i]:\n",
    "            #문서에서 선별한 단어(feature)의 위치를 찾아 임베딩 벡터 추출\n",
    "            frame1[idx_cnt]=np.load('./bert_embedding/test(bert)/test%d.npy'%i)[test_lst[i].index(feature_lst10000[j])]\n",
    "            idx_cnt+=1\n",
    "    textcuboid_test.append(frame1)\n",
    "textcuboid_test=np.array(textcuboid_test)\n",
    "np.save('./1-Channel textcuboid_test_r8(bert).npy',textcuboid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3071295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
